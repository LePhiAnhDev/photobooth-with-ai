import cv2
import mediapipe as mp
import numpy as np
import math
import time
import os
import asyncio
import base64
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
import uvicorn
from typing import Dict, List
import json

app = FastAPI()

# CORS middleware ƒë·ªÉ frontend c√≥ th·ªÉ k·∫øt n·ªëi
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class HandGestureRecognizer:
    def __init__(self):
        # Kh·ªüi t·∫°o MediaPipe v·ªõi settings t·ªëi ∆∞u
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,  # Ch·ªâ nh·∫≠n di·ªán 1 tay
            min_detection_confidence=0.8,  # ‚úÖ TƒÉng confidence ƒë·ªÉ ch√≠nh x√°c h∆°n
            min_tracking_confidence=0.7   # ‚úÖ TƒÉng confidence ƒë·ªÉ ch√≠nh x√°c h∆°n
        )
        
        # C√°c bi·∫øn ƒëi·ªÅu khi·ªÉn
        self.zoom_level = 1.0  # M·ª©c zoom hi·ªán t·∫°i
        self.min_zoom = 1.0    # Zoom t·ªëi thi·ªÉu
        self.max_zoom = 3.0    # Zoom t·ªëi ƒëa
        self.zoom_step = 0.2   # TƒÉng b∆∞·ªõc nh·∫£y zoom ƒë·ªÉ ph·∫£n h·ªìi nhanh h∆°n
        
        # Bi·∫øn mode v√† capture
        self.mode = "OFF"  # OFF ho·∫∑c ON
        self.is_capturing = False
        self.captured_photos = []
        self.max_photos = 6
        self.countdown = 0
        self.last_ok_detection = 0
        self.ok_cooldown = 1.0  # ‚úÖ Gi·∫£m cooldown xu·ªëng 1 gi√¢y ƒë·ªÉ d·ªÖ test
        self.last_countdown_update = 0
        self.countdown_interval = 1.0  # 1 gi√¢y gi·ªØa c√°c countdown
        self.requires_ok_continuous = False  # Kh√¥ng y√™u c·∫ßu OK sign li√™n t·ª•c
        
        # ‚úÖ TH√äM: Bi·∫øn ƒë·ªÉ ƒë·∫øm s·ªë l·∫ßn Peace sign li√™n t·ª•c
        self.peace_sign_count = 0
        self.required_peace_count = 3  # C·∫ßn 3 frame li√™n t·ª•c ƒë·ªÉ tr√°nh false positive
        self.last_gesture = 'unknown'
        self.gesture_stability_count = 0  # ƒê·∫øm s·ªë frame ·ªïn ƒë·ªãnh c·ªßa gesture
        
        # T·∫°o th∆∞ m·ª•c l∆∞u ·∫£nh n·∫øu ch∆∞a c√≥
        if not os.path.exists('captured_images'):
            os.makedirs('captured_images')

    def calculate_distance(self, point1, point2):
        """T√≠nh kho·∫£ng c√°ch gi·ªØa 2 ƒëi·ªÉm landmark"""
        return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2)

    def is_finger_up(self, landmarks, tip_id, pip_id):
        """Ki·ªÉm tra ng√≥n tay c√≥ du·ªói th·∫≥ng kh√¥ng"""
        return landmarks[tip_id].y < landmarks[pip_id].y
    
    def is_thumb_up(self, landmarks):
        """Ki·ªÉm tra ng√≥n c√°i c√≥ du·ªói th·∫≥ng kh√¥ng (x·ª≠ l√Ω ri√™ng cho ng√≥n c√°i)"""
        thumb_tip = 4
        thumb_ip = 3
        thumb_mcp = 2
        
        # Ki·ªÉm tra theo h∆∞·ªõng c·ªßa tay (tr√°i/ph·∫£i)
        # Ng√≥n c√°i du·ªói khi tip xa h∆°n so v·ªõi c√°c kh·ªõp kh√°c
        horizontal_extended = abs(landmarks[thumb_tip].x - landmarks[thumb_mcp].x) > abs(landmarks[thumb_ip].x - landmarks[thumb_mcp].x)
        vertical_extended = landmarks[thumb_tip].y < landmarks[thumb_ip].y
        
        # K·∫øt h·ª£p c·∫£ 2 ti√™u ch√≠ ƒë·ªÉ ch√≠nh x√°c h∆°n
        return horizontal_extended and vertical_extended

    def recognize_gesture(self, landmarks):
        """
        Nh·∫≠n di·ªán c·ª≠ ch·ªâ tay d·ª±a tr√™n landmarks
        Returns: 'fist', 'open', 'peace', ho·∫∑c 'unknown'
        """
        # C√°c ch·ªâ s·ªë landmark quan tr·ªçng
        thumb_tip = 4
        thumb_ip = 3
        index_tip = 8
        index_pip = 6
        middle_tip = 12
        middle_pip = 10
        ring_tip = 16
        ring_pip = 14
        pinky_tip = 20
        pinky_pip = 18

        # Ki·ªÉm tra c√°c ng√≥n tay c√≥ du·ªói th·∫≥ng kh√¥ng
        fingers_up = []
        
        # ‚úÖ Ng√≥n c√°i - s·ª≠ d·ª•ng logic c·∫£i ti·∫øn
        fingers_up.append(self.is_thumb_up(landmarks))
        
        # ‚úÖ 4 ng√≥n c√≤n l·∫°i - s·ª≠ d·ª•ng logic chu·∫©n
        for tip, pip in [(index_tip, index_pip), (middle_tip, middle_pip), 
                        (ring_tip, ring_pip), (pinky_tip, pinky_pip)]:
            fingers_up.append(self.is_finger_up(landmarks, tip, pip))

        fingers_up_count = fingers_up.count(True)

        # Lo·∫°i b·ªè debug log ƒë·ªÉ tƒÉng performance
        
        # ‚úÖ PEACE SIGN CH√çNH X√ÅC: D·∫•u ‚úåüèª (peace sign)
        # C√≥ th·ªÉ c√≥ 2 ho·∫∑c 3 ng√≥n du·ªói (ng√≥n c√°i c√≥ th·ªÉ du·ªói ho·∫∑c kh√¥ng)
        peace_condition_1 = (fingers_up_count == 2 and 
                            not fingers_up[0] and  # Ng√≥n c√°i c·ª•p
                            fingers_up[1] and      # Ng√≥n tr·ªè du·ªói  
                            fingers_up[2] and      # Ng√≥n gi·ªØa du·ªói
                            not fingers_up[3] and  # Ng√≥n √°p √∫t c·ª•p
                            not fingers_up[4])     # Ng√≥n √∫t c·ª•p
        
        peace_condition_2 = (fingers_up_count == 3 and 
                            fingers_up[0] and      # Ng√≥n c√°i du·ªói (c√≥ th·ªÉ)
                            fingers_up[1] and      # Ng√≥n tr·ªè du·ªói
                            fingers_up[2] and      # Ng√≥n gi·ªØa du·ªói
                            not fingers_up[3] and  # Ng√≥n √°p √∫t c·ª•p
                            not fingers_up[4])     # Ng√≥n √∫t c·ª•p
        
        if peace_condition_1 or peace_condition_2:
            return 'peace'

        # Nh·∫≠n di·ªán n·∫Øm tay: t·∫•t c·∫£ ng√≥n tay c·ª•p l·∫°i
        if fingers_up_count <= 1:
            return 'fist'
        
        # Nh·∫≠n di·ªán m·ªü tay: h·∫ßu h·∫øt ng√≥n tay du·ªói
        elif fingers_up_count >= 4:
            return 'open'
        
        return 'unknown'

    def apply_zoom(self, frame):
        """√Åp d·ª•ng zoom v√†o frame"""
        if self.zoom_level == 1.0:
            return frame
        
        h, w = frame.shape[:2]
        
        # T√≠nh to√°n v√πng crop ƒë·ªÉ zoom
        crop_h = int(h / self.zoom_level)
        crop_w = int(w / self.zoom_level)
        
        start_x = (w - crop_w) // 2
        start_y = (h - crop_h) // 2
        
        # Crop v√† resize v·ªÅ k√≠ch th∆∞·ªõc ban ƒë·∫ßu
        cropped = frame[start_y:start_y + crop_h, start_x:start_x + crop_w]
        zoomed = cv2.resize(cropped, (w, h))
        
        return zoomed

    def capture_image(self, frame):
        """Ch·ª•p v√† l∆∞u ·∫£nh"""
        if len(self.captured_photos) >= self.max_photos:
            return False
            
        timestamp = int(time.time() * 1000)
        filename = f'captured_images/capture_{timestamp}.jpg'
        cv2.imwrite(filename, frame)
        
        # Encode ·∫£nh th√†nh base64 ƒë·ªÉ g·ª≠i qua WebSocket (ch·∫•t l∆∞·ª£ng cao)
        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
        img_base64 = base64.b64encode(buffer).decode('utf-8')
        
        photo_data = {
            'id': str(timestamp),
            'dataUrl': f'data:image/jpeg;base64,{img_base64}',
            'timestamp': timestamp
        }
        
        self.captured_photos.append(photo_data)
        return True

    def process_frame(self, frame):
        """X·ª≠ l√Ω frame v√† tr·∫£ v·ªÅ th√¥ng tin c·∫ßn thi·∫øt - CLEAN VERSION"""
        # L·∫≠t frame theo chi·ªÅu ngang ƒë·ªÉ c√≥ c·∫£m gi√°c nh∆∞ nh√¨n g∆∞∆°ng
        frame = cv2.flip(frame, 1)
        
        # Resize frame ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω
        small_frame = cv2.resize(frame, (320, 240))
        rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        
        # X·ª≠ l√Ω nh·∫≠n di·ªán tay
        results = self.hands.process(rgb_frame)
        gesture = 'unknown'
        current_time = time.time()
        
        # X·ª≠ l√Ω khi c√≥ tay ƒë∆∞·ª£c ph√°t hi·ªán
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                gesture = self.recognize_gesture(hand_landmarks.landmark)
                
                # X·ª≠ l√Ω c√°c gesture
                if gesture == 'fist':
                    if self.zoom_level > self.min_zoom:
                        self.zoom_level = max(self.min_zoom, self.zoom_level - self.zoom_step)
                        
                elif gesture == 'open':
                    if self.zoom_level < self.max_zoom:
                        self.zoom_level = min(self.max_zoom, self.zoom_level + self.zoom_step)
                        
                elif gesture == 'peace':
                    self._handle_peace_sign(current_time)
                
                # Reset counters cho c√°c gesture kh√°c
                if gesture != 'peace' and gesture != self.last_gesture:
                    self.peace_sign_count = 0
                    self.gesture_stability_count = 0
        
        # C·∫≠p nh·∫≠t last_gesture
        self.last_gesture = gesture
        
        # X·ª≠ l√Ω countdown v√† ch·ª•p ·∫£nh
        self._handle_countdown_and_capture(current_time, frame)
        
        # √Åp d·ª•ng zoom v√† chu·∫©n b·ªã output
        display_frame = self.apply_zoom(frame)
        _, buffer = cv2.imencode('.jpg', display_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        frame_base64 = base64.b64encode(buffer).decode('utf-8')
        
        return {
            'frame': f'data:image/jpeg;base64,{frame_base64}',
            'gesture': gesture,
            'zoom_level': round(self.zoom_level, 1),
            'mode': self.mode,
            'is_capturing': self.is_capturing,
            'countdown': self.countdown,
            'captured_photos': self.captured_photos,
            'photos_count': len(self.captured_photos),
            'peace_sign_count': self.peace_sign_count,
            'required_peace_count': self.required_peace_count,
            'gesture_stability_count': self.gesture_stability_count,
            'gesture_stability_required': self.required_peace_count
        }
    
    def _handle_peace_sign(self, current_time):
        """X·ª≠ l√Ω logic Peace Sign - CLEAN VERSION"""
        # ƒê·∫øm stability
        if self.last_gesture == 'peace':
            self.gesture_stability_count += 1
        else:
            self.gesture_stability_count = 1
        
        # K√≠ch ho·∫°t khi ƒë·ªß ƒëi·ªÅu ki·ªán
        if (self.gesture_stability_count >= self.required_peace_count and 
            current_time - self.last_ok_detection > self.ok_cooldown and
            self.mode == "OFF" and not self.is_capturing):
            
            self.last_ok_detection = current_time
            self.mode = "ON"
            self.is_capturing = True
            self.countdown = 5
            self.last_countdown_update = current_time
            self.peace_sign_count = 0
            self.gesture_stability_count = 0
    
    def _handle_countdown_and_capture(self, current_time, frame):
        """X·ª≠ l√Ω countdown v√† ch·ª•p ·∫£nh"""
        if self.is_capturing and self.countdown > 0:
            if current_time - self.last_countdown_update >= self.countdown_interval:
                self.countdown -= 1
                self.last_countdown_update = current_time
                
                if self.countdown == 0:
                    if self.capture_image(frame):
                        if len(self.captured_photos) < self.max_photos:
                            self.countdown = 5
                            self.last_countdown_update = current_time
                        else:
                            self._reset_capture_mode()
                    else:
                        self._reset_capture_mode()
    
    def _reset_capture_mode(self):
        """Reset capture mode v·ªÅ OFF"""
        self.is_capturing = False
        self.mode = "OFF"
        self.countdown = 0
        self.peace_sign_count = 0
        self.gesture_stability_count = 0

# Kh·ªüi t·∫°o recognizer
recognizer = HandGestureRecognizer()

# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                pass

manager = ConnectionManager()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    
    # Kh·ªüi t·∫°o camera v·ªõi settings c√¢n b·∫±ng ch·∫•t l∆∞·ª£ng v√† t·ªëc ƒë·ªô
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # ƒê·ªô ph√¢n gi·∫£i cao cho ch·∫•t l∆∞·ª£ng t·ªët
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # ƒê·ªô ph√¢n gi·∫£i cao cho ch·∫•t l∆∞·ª£ng t·ªët
    cap.set(cv2.CAP_PROP_FPS, 30)            # FPS ·ªïn ƒë·ªãnh
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)      # Gi·∫£m buffer ƒë·ªÉ gi·∫£m delay
    
    if not cap.isOpened():
        await websocket.send_text(json.dumps({"error": "Cannot open camera"}))
        return
    
    try:
        while True:
            success, frame = cap.read()
            if not success:
                break
            
            # X·ª≠ l√Ω frame
            result = recognizer.process_frame(frame)
            
            # G·ª≠i k·∫øt qu·∫£ qua WebSocket
            await websocket.send_text(json.dumps(result))
            
            # Delay nh·ªè ƒë·ªÉ tr√°nh qu√° t·∫£i (FPS ·ªïn ƒë·ªãnh)
            await asyncio.sleep(0.033)  # ~30 FPS
            
    except (WebSocketDisconnect, Exception):
        pass
    finally:
        cap.release()
        manager.disconnect(websocket)

@app.post("/toggle_mode")
async def toggle_mode():
    """API ƒë·ªÉ toggle mode ON/OFF"""
    current_time = time.time()
    
    if recognizer.mode == "OFF" and not recognizer.is_capturing:
        recognizer.mode = "ON"
        recognizer.is_capturing = True
        recognizer.countdown = 5
        recognizer.last_countdown_update = current_time
    elif recognizer.mode == "ON" or recognizer.is_capturing:
        recognizer.mode = "OFF"
        recognizer.is_capturing = False
        recognizer.countdown = 0
    
    return {
        "mode": recognizer.mode,
        "is_capturing": recognizer.is_capturing,
        "countdown": recognizer.countdown
    }

@app.get("/status")
async def get_status():
    """API ƒë·ªÉ l·∫•y tr·∫°ng th√°i hi·ªán t·∫°i"""
    return {
        "mode": recognizer.mode,
        "zoom_level": recognizer.zoom_level,
        "is_capturing": recognizer.is_capturing,
        "countdown": recognizer.countdown,
        "photos_count": len(recognizer.captured_photos),
        "max_photos": recognizer.max_photos
    }

@app.post("/reset")
async def reset_photos():
    """API ƒë·ªÉ reset t·∫•t c·∫£ ·∫£nh ƒë√£ ch·ª•p"""
    recognizer.captured_photos = []
    recognizer.mode = "OFF"
    recognizer.is_capturing = False
    recognizer.countdown = 0
    recognizer.last_ok_detection = 0
    recognizer.last_countdown_update = 0
    recognizer.peace_sign_count = 0
    recognizer.last_gesture = 'unknown'
    recognizer.gesture_stability_count = 0
    return {"message": "Reset successful"}

@app.get("/")
async def root():
    return {"message": "Photobooth AI Backend is running!"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
