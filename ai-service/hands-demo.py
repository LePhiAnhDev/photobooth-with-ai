import cv2
import mediapipe as mp
import numpy as np
import math
import time
import os

class HandGestureRecognizer:
    def __init__(self):
        # Kh·ªüi t·∫°o MediaPipe
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,  # Ch·ªâ nh·∫≠n di·ªán 1 tay
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.mp_draw = mp.solutions.drawing_utils
        
        # C√°c bi·∫øn ƒëi·ªÅu khi·ªÉn
        self.zoom_level = 1.0  # M·ª©c zoom hi·ªán t·∫°i
        self.min_zoom = 1.0    # Zoom t·ªëi thi·ªÉu
        self.max_zoom = 3.0    # Zoom t·ªëi ƒëa
        self.zoom_step = 0.1   # B∆∞·ªõc nh·∫£y zoom
        
        # Bi·∫øn ƒë·ªÉ tr√°nh ch·ª•p ·∫£nh li√™n t·ª•c
        self.last_capture_time = 0
        self.capture_cooldown = 2.0  # 2 gi√¢y cooldown
        
        # T·∫°o th∆∞ m·ª•c l∆∞u ·∫£nh n·∫øu ch∆∞a c√≥
        if not os.path.exists('captured_images'):
            os.makedirs('captured_images')

    def calculate_distance(self, point1, point2):
        """T√≠nh kho·∫£ng c√°ch gi·ªØa 2 ƒëi·ªÉm landmark"""
        return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2)

    def is_finger_up(self, landmarks, tip_id, pip_id):
        """Ki·ªÉm tra ng√≥n tay c√≥ du·ªói th·∫≥ng kh√¥ng"""
        return landmarks[tip_id].y < landmarks[pip_id].y

    def recognize_gesture(self, landmarks):
        """
        Nh·∫≠n di·ªán c·ª≠ ch·ªâ tay d·ª±a tr√™n landmarks
        Returns: 'fist', 'open', 'ok', ho·∫∑c 'unknown'
        """
        # C√°c ch·ªâ s·ªë landmark quan tr·ªçng
        thumb_tip = 4
        thumb_ip = 3
        index_tip = 8
        index_pip = 6
        middle_tip = 12
        middle_pip = 10
        ring_tip = 16
        ring_pip = 14
        pinky_tip = 20
        pinky_pip = 18

        # Ki·ªÉm tra c√°c ng√≥n tay c√≥ du·ªói th·∫≥ng kh√¥ng
        fingers_up = []
        
        # Ng√≥n c√°i (ki·ªÉm tra theo chi·ªÅu ngang)
        if landmarks[thumb_tip].x > landmarks[thumb_ip].x:  # Tay ph·∫£i
            fingers_up.append(landmarks[thumb_tip].x > landmarks[thumb_ip].x)
        else:  # Tay tr√°i
            fingers_up.append(landmarks[thumb_tip].x < landmarks[thumb_ip].x)
        
        # 4 ng√≥n c√≤n l·∫°i
        for tip, pip in [(index_tip, index_pip), (middle_tip, middle_pip), 
                        (ring_tip, ring_pip), (pinky_tip, pinky_pip)]:
            fingers_up.append(self.is_finger_up(landmarks, tip, pip))

        fingers_up_count = fingers_up.count(True)

        # Nh·∫≠n di·ªán c·ª≠ ch·ªâ OK: ng√≥n c√°i v√† ng√≥n tr·ªè ch·∫°m nhau, c√°c ng√≥n kh√°c du·ªói
        thumb_index_distance = self.calculate_distance(landmarks[thumb_tip], landmarks[index_tip])
        
        if thumb_index_distance < 0.05:  # Ng√≥n c√°i v√† tr·ªè g·∫ßn nhau
            if fingers_up[2] and fingers_up[3] and fingers_up[4]:  # 3 ng√≥n c√≤n l·∫°i du·ªói
                return 'ok'

        # Nh·∫≠n di·ªán n·∫Øm tay: t·∫•t c·∫£ ng√≥n tay c·ª•p l·∫°i
        if fingers_up_count <= 1:
            return 'fist'
        
        # Nh·∫≠n di·ªán m·ªü tay: h·∫ßu h·∫øt ng√≥n tay du·ªói
        elif fingers_up_count >= 4:
            return 'open'
        
        return 'unknown'

    def apply_zoom(self, frame):
        """√Åp d·ª•ng zoom v√†o frame"""
        if self.zoom_level == 1.0:
            return frame
        
        h, w = frame.shape[:2]
        
        # T√≠nh to√°n v√πng crop ƒë·ªÉ zoom
        crop_h = int(h / self.zoom_level)
        crop_w = int(w / self.zoom_level)
        
        start_x = (w - crop_w) // 2
        start_y = (h - crop_h) // 2
        
        # Crop v√† resize v·ªÅ k√≠ch th∆∞·ªõc ban ƒë·∫ßu
        cropped = frame[start_y:start_y + crop_h, start_x:start_x + crop_w]
        zoomed = cv2.resize(cropped, (w, h))
        
        return zoomed

    def capture_image(self, frame):
        """Ch·ª•p v√† l∆∞u ·∫£nh"""
        current_time = time.time()
        if current_time - self.last_capture_time > self.capture_cooldown:
            timestamp = int(current_time)
            filename = f'captured_images/capture_{timestamp}.jpg'
            cv2.imwrite(filename, frame)
            print(f"üì∏ ƒê√£ ch·ª•p ·∫£nh: {filename}")
            self.last_capture_time = current_time
            return True
        return False

    def run(self):
        """Ch·∫°y ch∆∞∆°ng tr√¨nh ch√≠nh"""
        # Kh·ªüi t·∫°o camera
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("‚ùå Kh√¥ng th·ªÉ m·ªü camera!")
            return
        
        print("üöÄ B·∫Øt ƒë·∫ßu nh·∫≠n di·ªán c·ª≠ ch·ªâ tay...")
        print("üìã H∆∞·ªõng d·∫´n:")
        print("   üëä N·∫Øm tay ‚Üí Zoom Out")
        print("   ‚úã M·ªü tay ‚Üí Zoom In") 
        print("   üëå OK sign ‚Üí Ch·ª•p ·∫£nh")
        print("   ‚å®Ô∏è  Nh·∫•n 'q' ƒë·ªÉ tho√°t")
        
        while True:
            success, frame = cap.read()
            if not success:
                print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera!")
                break
            
            # L·∫≠t frame theo chi·ªÅu ngang ƒë·ªÉ c√≥ c·∫£m gi√°c nh∆∞ nh√¨n g∆∞∆°ng
            frame = cv2.flip(frame, 1)
            h, w, c = frame.shape
            
            # Chuy·ªÉn ƒë·ªïi BGR sang RGB cho MediaPipe
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # X·ª≠ l√Ω nh·∫≠n di·ªán tay
            results = self.hands.process(rgb_frame)
            
            gesture = 'unknown'
            
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    # V·∫Ω landmarks l√™n frame
                    self.mp_draw.draw_landmarks(
                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS,
                        self.mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2),
                        self.mp_draw.DrawingSpec(color=(255, 0, 0), thickness=2)
                    )
                    
                    # Nh·∫≠n di·ªán c·ª≠ ch·ªâ
                    gesture = self.recognize_gesture(hand_landmarks.landmark)
                    
                    # X·ª≠ l√Ω c√°c h√†nh ƒë·ªông d·ª±a tr√™n c·ª≠ ch·ªâ
                    if gesture == 'fist':
                        # Zoom out
                        if self.zoom_level > self.min_zoom:
                            self.zoom_level = max(self.min_zoom, self.zoom_level - self.zoom_step)
                    
                    elif gesture == 'open':
                        # Zoom in
                        if self.zoom_level < self.max_zoom:
                            self.zoom_level = min(self.max_zoom, self.zoom_level + self.zoom_step)
                    
                    elif gesture == 'ok':
                        # Ch·ª•p ·∫£nh
                        if self.capture_image(frame):
                            # Hi·ªáu ·ª©ng flash khi ch·ª•p
                            flash_frame = np.ones_like(frame) * 255
                            cv2.imshow('Hand Gesture Control', flash_frame)
                            cv2.waitKey(100)
            
            # √Åp d·ª•ng zoom
            display_frame = self.apply_zoom(frame)
            
            # Hi·ªÉn th·ªã th√¥ng tin tr√™n m√†n h√¨nh
            info_text = [
                f"Gesture: {gesture}",
                f"Zoom: {self.zoom_level:.1f}x",
                "Press 'q' to quit"
            ]
            
            for i, text in enumerate(info_text):
                y_pos = 30 + i * 30
                cv2.putText(display_frame, text, (10, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                cv2.putText(display_frame, text, (10, y_pos), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)
            
            # Hi·ªÉn th·ªã frame
            cv2.imshow('Hand Gesture Control', display_frame)
            
            # Tho√°t khi nh·∫•n 'q'
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        # D·ªçn d·∫πp
        cap.release()
        cv2.destroyAllWindows()
        print("üëã Ch∆∞∆°ng tr√¨nh ƒë√£ k·∫øt th√∫c!")

def main():
    """H√†m main ƒë·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh"""
    try:
        recognizer = HandGestureRecognizer()
        recognizer.run()
    except KeyboardInterrupt:
        print("\n‚ö° Ch∆∞∆°ng tr√¨nh b·ªã ng·∫Øt b·ªüi ng∆∞·ªùi d√πng!")
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        print("üí° H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ c√†i ƒë·∫∑t: pip install opencv-python mediapipe numpy")

if __name__ == "__main__":
    main()